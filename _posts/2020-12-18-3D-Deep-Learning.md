---
layout: post
title:  "A Survey of Deep Learning Methods for 3D Object Classification"
date:   2020-12-18 18:05:55 +0300
image:  '/assets/img/data.jpg'
tags:   CV 
---
# Abstract
The objective of this study was to survey different deep learning techniques to analyze 3D data for object classification. Understanding 3D data is attracting attention due to its various applications from autonomous driving to medical image processing. Unlike 2D data, which has been extensively researched, traditional classification techniques on 3D data lack robustness. However, due to recent advances and lower costs, 3D data is gaining abundance and driving innovation in the 3D space. The various approaches surveyed in this study all revolve around the utilization of deep learning algorithms. These techniques are based on different 3D data representations such as multi-view RGB-D data, volumetric data, and point cloud data. This study implements a convolutional neural network on a 3D point cloud dataset of handwritten digits. The same convolutional neural network architecture is then compared to a similar 2D version of the dataset to compare accuracy results. The goals of the study are to understand the recent advancements and current state of 3D object recognition.

# Introduction
Deep learning has proven itself to be a very powerful tool for developing machine learning models in both one and two dimensions. In one-dimension, deep learning is applied to time-series data to model a broad range of applications from language to finance. Similarly, in two-dimensions, deep learning has been extensively researched in images where a 2D matrix of an image is passed to a deep learning model. One of the key strengths of deep learning is its ability to discriminate, detect, and correlate features on its own which enables faster learning without explicit supervision. As a result, this powerful tool has been extensively researched and is already established on 2D data. 

Nevertheless, we live in a 3D world and increasing attention is being placed on deep learning techniques to model various aspects of the world around us. Historically, most 3D models lacked robustness due to a shortage of 3D data. Due to this reason, applying deep learning techniques on 3D data was not as effective as on traditional 1D or 2D domains. However, recent development of cameras and scanning devices, like Intel’s LIDAR camera and Microsoft’s Kinect sensor, are leading development in 3D deep learning by making data easier to obtain. The surge in 3D deep learning started in 2015 along with the distribution of key 3D datasets: ShapeNet and ModelNet. The introduction of numerous open source 3D datasets was the impetus behind research to some of the most complex 3D problems. In autonomous driving, deep learning is utilized extensively by the automotive industry to model perception. Similarly, deep learning is used by medical practitioners in 3D medical image processing. 

While there are many applications for 3D deep learning they usually fall into one of three tasks: 3D geometry analysis, 3D-assisted image analysis, and 3D synthesis. 3D geometry analysis encompasses the realms of object classification, parsing, and correspondence. 3D synthesis involves the reconstruction of objects from 2D images, shape completion, and shape modeling. 3D-assisted images analysis deals with image retrieval and decomposition. These various subfields of 3D deep learning rely on different data representations. In particular, this study focuses on multi-view representations, volumetric representations, RGB-D data, and point clouds for object classification.

# Multi-View Data
Multi-view data is represented as a combination of 2D images captured from multiple points of view. These images are used to capture the 3D object as a whole. Deep learning is applied to each point of view individually and features are extracted. Learned features are then merged together through view pooling. View-pooling is used to process multiple views for the 3D object in no specific order. The output of view-pooling is then passed through a deep learning neural network for a final prediction of our 3D object. Through back-propagation, we can fine tune our network parameters further. 

## MVCNN
According to Su et al. in [1], building deep learning classifiers for 3D models upon 2D image renderings dramatically outperformed classifiers built directly on 3D representations. One reason for the outperformance is that 3D representations rely on a significant reduction in resolution. On the other hand, 2D image renderings can be trained with deep learning quickly without the need for changes in resolution. Su et al. applied a multi-view convolutional neural network on ModelNet40, a comprehensive dataset of 3D CAD models of objects, and found that their model outperformed traditional 3D models. Their multi-view convolutional neural network (MVCNN) was able to achieve 89.9% accuracy with 12 views and 90.1% accuracy with 80 views for classification tasks. This is in contrast to the 3D ShapeNet’s algorithm developed by Wu et al. in [3] which only achieved a 77.3% classification accuracy.

Another advantage of a MVCNN is the ubiquity of 2D image datasets. Deep learning models can learn a great deal of generic features from 2D image renderings and then be fine-tuned for specific classification problems on 3D projections. Unlike 2D images, large labeled 3D datasets are not abundant and therefore suffer as a result in terms of accuracy. Moreover, 2D image renderings allow for transformations to reduce noise, occlusion, and illumination. 

One concern with the multi-view representation is the optimal number of views or 2D rendered images that should be used to build the deep learning classifier. Too few views could lead to inaccurate results, as the views don't make up a true representation of the 3D object. In contrast, too many views cause excessive computational overhead and could lead to an overfitted model. Multi-view representations add another parameter that must be optimized in order to get accurate classification results.

## MVCNN with VGG-11
Su et al. [2] built off of the MVCNN in [1] with a modification to the rendering technique and deeper architectures. Their algorithm improved the accuracy of the MVCNN to 95% per-instance accuracy on the ModelNet40 dataset. One reason for their algorithms' strong performance is due to pretraining. To model the accuracy results of pretraining of MVCNN, Su et al. [2] measured the accuracy of two VGG-11 architectures with and without ImageNet pretraining. VGG or Visual Geometry Group is a pretrained deep convolutional neural network used for large-scale image recognition tasks. VGG-11 with ImageNet pretraining achieved a per instance accuracy of 95% compared to 91.3% without ImageNet pretraining. Su et al. [2] also implemented shaded image renderings for stronger accuracy results. Shaded image renderings were found to achieve 3.4% higher accuracy results compared to depth images and 1.4% accuracy improvement compared to binary silhouette images.

Despite the need to optimize the number of views, multi-view data can be sufficiently used to classify 3D objects. They are suitable to model rigid 3D objects and allow for generalization towards other 3D features. In addition, multi-view data proves more effective in learning 3D features than volumetric representations such as ShapeNet.

<p align="center">
  <img src="/assets/img/Table1.PNG" />
</p>

# Volumetric Data
Another way 3D data can be represented is by a regular grid in 3D space. Just how pixels are the smallest element of a 2D image rendering, a voxel is a basic 3D unit that characterizes the volume in three dimensions. The process of converting geometric objects into a set of voxels is called voxelization. Voxelation can be used to create 3D computer aided design (CAD) models, model the brain in three dimensions from CT and fMRI scans, and model sediment layers for geology. The voxelized object is then run through a 3D convolutional neural network or volumetric convolution. 3D convolutions utilize a kernel that moves in 3 directions (x, y, z) with input and output data in four dimensions. 

As one can imagine, running 3D convolutions with four dimensional kernels can be computationally expensive. A more efficient volumetric approach is octree generating networks. Coined by Tatarchenko et al. in [5], Octree-based networks model 3D objects as a hierarchical structure. A quadtree structure recursively decomposes voxels to divide the scene into cubes that are inside or outside the 3D object. This approach is better able to capture fine details of a 3D object with less computational intensity than a voxel-based approach.

## ShapeNet
One of the very first algorithms to exploit the geometry of voxels was ShapeNet. Developed by Wu et al. in [3], ShapeNet models 3D objects as a probability distribution of binary variables on a 3D voxel grid. ShapeNet utilizes a Convolutional Deep Belief Network, a type of deep learning neural network that was adopted from 2D deep learning to model 3D objects, to represent the probability distribution of binary variables. ShapeNet consists of five layers: an input layer, 3 convolutional layers, and an output layer. These layers are pre-trained using multiple methods. Despite ShapeNet’s simplistic strategy to classify 3D objects based on volumetric data, it suffers from memory storage limitations and information loss. Due to the additional dimension of the convolutional layer, ShapeNet cannot process large amounts of high-resolution data. In contrast, if the resolution is too low, more points fall into a voxel, which increases the information loss. Therefore, finding an optimal resolution is critical to minimize the information loss and computational time of volumetric-based algorithms.

## VoxNet
Building upon the concept of 3D convolutions, Maturana and Scherer in [4] developed VoxNet. The VoxNet architecture consists of an input layer which can take different 3D data representations, like RGB-D, LIBAR point clouds, and 3D CAD models, as input. The input layer is constructed as a volumetric occupancy grid of 32x32x32 voxels. VoxNet then runs the input through two convolutional layers, a pooling layer, and two fully connected layers. In comparison to ShapeNet, VoxNet outperforms ShapeNet on the ModelNet40 dataset with an accuracy of 83% compared to ShapeNet’s accuracy of 77.3%.

## LightNet
In light of the huge computational requirements of volumetric-based approaches, Zhi et al. proposed LightNet in [6]. LightNet is a lightweight 3D convolutional neural network for real-time 3D object recognition. Based on VoxNet, LightNet leverages multitask learning to improve accuracy and efficiency by learning multiple features at the same time. Moreover, LightNet utilizes a batch normalization operation between the convolutional and activation operations to achieve faster convergence. In terms of accuracy, LightNet trained from scratch achieved an accuracy of 82.9% on ModelNet40. This score increases dramatically if LightNet is pretrained on ModelNet10 with an accuracy of 86.9% on ModelNet40. In addition to increased accuracy, LightNet is able to classify a 3D object within 5ms. This means that LightNet can be utilized for real-time object recognition tasks.

While volumetric-based representations like ShapeNet, VoxNet, and LightNet are simple, they struggle in terms of accuracy and computation time compared to multi-view representations. Despite their limitations, volumetric-based approaches prove to be effective for real-time object recognition tasks where recognition time is of the essence.

<p align="center">
  <img src="/assets/img/Table2.PNG" />
</p>